---
title: "Clustering"
author: "Dr. Marko Mitic"
output: pdf_document
---

## Problem 1: **DOCUMENT CLUSTERING WITH DAILY KOS**

Document clustering, or text clustering, is a very popular application of clustering algorithms. A web search engine, like Google, often returns thousands of results for a simple query. For example, if you type the search term "jaguar" into Google, around 200 million results are returned. This makes it very difficult to browse or find relevant information, especially if the search term has multiple meanings. If we search for "jaguar", we might be looking for information about the animal, the car, or the Jacksonville Jaguars football team. 

Clustering methods can be used to automatically group search results into categories, making it easier to find relavent results. This method is used in the search engines PolyMeta and Helioid, as well as on FirstGov.gov, the official Web portal for the U.S. government. The two most common algorithms used for document clustering are Hierarchical and k-means. 

In this problem, we'll be clustering articles published on Daily Kos <https://www.dailykos.com/>, an American political blog that publishes news and opinion articles written from a progressive point of view. Daily Kos was founded by Markos Moulitsas in 2002, and as of September 2014, the site had an average weekday traffic of hundreds of thousands of visits. 

The file dailykos.csv contains data on 3,430 news articles or blogs that have been posted on Daily Kos. These articles were posted in 2004, leading up to the United States Presidential Election. The leading candidates were incumbent President George W. Bush (republican) and John Kerry (democratic). Foreign policy was a dominant topic of the election, specifically, the 2003 invasion of Iraq. 

Each of the variables in the dataset is a word that has appeared in at least 50 different articles (1,545 words in total). The set of  words has been trimmed according to some of the techniques covered in the previous week on text analytics (punctuation has been removed, and stop words have been removed). For each document, the variable values are the number of times that word appeared in the document. 


Let's start by building a hierarchical clustering model. 

```{r}
dailykos = read.csv("dailykos.csv")
str(dailykos)
distance = dist(dailykos, method = "euclidean")
cluster = hclust(distance, method = "ward.D")
```

The computation may take some time, since we have lots of observations and variables in the dataset. Let us next plot the dendogram:
```{r}
plot(cluster)
```

The choices 2 and 3 are good cluster choices according to the dendrogram, because there is a lot of space between the horizontal lines in the dendrogram in those cut off spots (draw a horizontal line across the dendrogram where it crosses 2 or 3 vertical lines). This can be shown by using `rect.hist` function for drawing cluster:

```{r}
plot(cluster)
rect.hclust(cluster, k=3, border="red")
```

However, just thinking about the application, it is probably better to show the reader more categories than 2 or 3. These categories would probably be too broad to be useful. Seven or eight categories seems more reasonable. Let us next subset each of the seven clusters:

```{r}
clusterGroups =cutree(cluster, k = 7)
cluster1 = subset(dailykos, clusterGroups == 1)
cluster2 = subset(dailykos, clusterGroups == 2)
cluster3 = subset(dailykos, clusterGroups == 3)
cluster4 = subset(dailykos, clusterGroups == 4)
cluster5 = subset(dailykos, clusterGroups == 5)
cluster6 = subset(dailykos, clusterGroups == 6)
cluster7 = subset(dailykos, clusterGroups == 7)
```

By using `str` function we observe that cluster1 contains most observations, while cluster4 has lowest number of them. We can also see the frequency for each varaible in each cluster. Combination of `tail`, `sort` and `colMeans` computes the mean frequency values of each of the words in cluster, and then outputs the 6 words that occur the most frequently. The colMeans function computes the column (word) means, the sort function orders the words in increasing order of the mean values, and the tail function outputs the last 6 words listed, which are the ones with the largest column means.

```{r}
tail(sort(colMeans(cluster1)))
```

We observe that the word "bush" is most frequent word in this cluster. For cluster 2 this are words "november" and "poll".

```{r}
tail(sort(colMeans(cluster2)))
```

Next, we can run k-means algorithm, to find new patterns.

```{r}
k = 7 #seven clusters
set.seed(1000)
KMC = kmeans(dailykos, centers = k)
```

We now subset the `KMC`, as in hierarchical clustering:

```{r}
dailykosClusters = KMC$cluster
```

The number observations in each cluster can be determined using sum function:
```{r}
sum(dailykosClusters==1)
sum(dailykosClusters==2)
sum(dailykosClusters==3)
sum(dailykosClusters==4)
sum(dailykosClusters==5)
sum(dailykosClusters==6)
sum(dailykosClusters==7)

# or using:
KmeansCluster = split(dailykos, dailykosClusters)
#str(KmeansCluster)
```

It can be observed that cluster 4 and cluster 2 have largest and smallest number of observations. This is, of course, different comparing hierarchical clustering case. Most frequent terms can also be obtained using cobination of `tail`, `sort` and `colMeans`:

```{r}
KmeansCluster1 = subset(dailykos, KMC$cluster == 1)
KmeansCluster2 = subset(dailykos, KMC$cluster == 2)
KmeansCluster3 = subset(dailykos, KMC$cluster == 3)
KmeansCluster4 = subset(dailykos, KMC$cluster == 4)
KmeansCluster5 = subset(dailykos, KMC$cluster == 5)
KmeansCluster6 = subset(dailykos, KMC$cluster == 6)
KmeansCluster7 = subset(dailykos, KMC$cluster == 7)
tail(sort(colMeans(KmeansCluster1)))
```

Comparing these results with hierarchical clustering, we can determine the similarity of each cluster. For example, using the `table` function, we observe that the hierarchical cluster 7 is most similar to K-means cluster 2:

```{r}
table(clusterGroups, KMC$cluster)
```

Similarly, it is interesting to note that K-means cluster 6 is almost identical to hierarchical cluster 2. We can also conclude that no more than 123 (39.9%) of the observations in K-Means Cluster 7 fall in any hierarchical cluster.

########################################################################

## Problem 2: **MARKET SEGMENTATION FOR AIRLINES**

Market segmentation is a strategy that divides a broad target market of customers into smaller, more similar groups, and then designs a marketing strategy specifically for each group. Clustering is a common technique for market segmentation since it automatically finds similar groups given a data set. 

In this problem, we'll see how clustering can be used to find similar groups of customers who belong to an airline's frequent flyer program. The airline is trying to learn more about its customers so that it can target different customer segments with different types of mileage offers. 

The file AirlinesCluster.csv contains information on 3,999 members of the frequent flyer program. This data comes from the textbook "Data Mining for Business Intelligence," by Galit Shmueli, Nitin R. Patel, and Peter C. Bruce. For more information, see the website for the book <http://www.dataminingbook.com/>.

There are seven different variables in the dataset, described below:

- **Balance** = number of miles eligible for award travel
- **QualMiles** = number of miles qualifying for TopFlight status
- **BonusMiles** = number of miles earned from non-flight bonus transactions in the past 12 months
- **BonusTrans** = number of non-flight bonus transactions in the past 12 months
- **FlightMiles** = number of flight miles in the past 12 months
- **FlightTrans** = number of flight transactions in the past 12 months
- **DaysSinceEnroll** = number of days since enrolled in the frequent flyer program

First,let's load the dataset and look at statistical summary:
```{r}
airlines = read.csv("AirlinesCluster.csv")
str(airlines)
summary(airlines)
```

It is obious that firstly we need to normalize the data. If we don't normalize the data, the clustering will be dominated by the variables that are on a larger scale.This is done next in our analysis:

```{r}
#install.packages("caret")
library(caret)
preproc = preProcess(airlines)
airlinesNorm = predict(preproc, airlines)
summary(airlinesNorm)
```
One can see from the output that FlightMiles now has the largest maximum value, and DaysSinceEnroll now has the smallest minimum value. Note that these were not the variables with the largest and smallest values in the original dataset airlines. Next, we're going to develop hierarchical clustering model:

```{r}
distance = dist(airlinesNorm, method = "euclidean")
HierCluster = hclust(distance, method = "ward.D")
plot(HierCluster)
```

Looking at the denddogram, we can decide that total number of clusters is from 2 to 7. In the next analysis we'll use k=5 clusters. We can subset the data for each of the cluster as follows:

```{r}
clusterGroups =cutree(HierCluster, k = 5)
HierCluster1 = subset(airlinesNorm, clusterGroups == 1)
HierCluster2 = subset(airlinesNorm, clusterGroups == 2)
HierCluster3 = subset(airlinesNorm, clusterGroups == 3)
HierCluster4 = subset(airlinesNorm, clusterGroups == 4)
HierCluster5 = subset(airlinesNorm, clusterGroups == 5)
```

We can use `lapply` to compare the average values in each of the variables for the 5 clusters (the centroids of the clusters):
```{r}
colMeans(subset(airlines, clusterGroups == 1))
colMeans(subset(airlines, clusterGroups == 2))
colMeans(subset(airlines, clusterGroups == 3))
colMeans(subset(airlines, clusterGroups == 4))
colMeans(subset(airlines, clusterGroups == 5))

lapply(split(airlines, clusterGroups), colMeans)
```

We also want to analyze the data using K-means algorithm as follows:
```{r}
k = 5 #five clusters
set.seed(88)
KMC = kmeans(airlinesNorm, centers = k, iter.max = 1000)
```

The number of observations in each cluster is easily determined:
```{r}
sum(KMC$cluster==1)
sum(KMC$cluster==2)
sum(KMC$cluster==3)
sum(KMC$cluster==4)
sum(KMC$cluster==5)

# or table(KMC$cluster)
```

We can compare the cluster centroids to each other either by dividing the data points into groups and then using tapply, or by looking at the output of kmeansClust$centers, where "kmeansClust" is the name of the output of the kmeans function.

```{r}
colMeans(subset(airlines, KMC$cluster == 1))
colMeans(subset(airlines, KMC$cluster == 2))
colMeans(subset(airlines, KMC$cluster == 3))
colMeans(subset(airlines, KMC$cluster == 4))
colMeans(subset(airlines, KMC$cluster == 5))

lapply(split(airlines, KMC$cluster), colMeans)
```

The clusters are not displayed in a meaningful order, so while there may be a cluster produced by the k-means algorithm that is similar to Cluster 1 produced by the Hierarchical method, it will not necessarily be shown first.

## Problem 3: **PREDICTING STOCK RETURNS WITH CLUSTER-THEN-PREDICT**

We'll use cluster-then-predict to predict future stock prices using historical stock data.

When selecting which stocks to invest in, investors seek to obtain good future returns. In this problem, we will first use clustering to identify clusters of stocks that have similar returns over time. Then, we'll use logistic regression to predict whether or not the stocks will have positive future returns.

For this problem, we'll use StocksCluster.csv, which contains monthly stock returns from the NASDAQ stock exchange. The NASDAQ is the second-largest stock exchange in the world, and it lists many technology companies. The stock price data used in this problem was obtained from infochimps <http://www.infochimps.com/>, a website providing access to many datasets.

Each observation in the dataset is the monthly returns of a particular company in a particular year. The years included are 2000-2009. The companies are limited to tickers that were listed on the exchange for the entire period 2000-2009, and whose stock price never fell below $1. So, for example, one observation is for Yahoo in 2000, and another observation is for Yahoo in 2001. Our goal will be to predict whether or not the stock return in December will be positive, using the stock returns for the first 11 months of the year.

This dataset contains the following variables:

- **ReturnJan** = the return for the company's stock during January (in the year of the observation). 
- **ReturnFeb** = the return for the company's stock during February (in the year of the observation). 
- **ReturnMar** = the return for the company's stock during March (in the year of the observation). 
- **ReturnApr** = the return for the company's stock during April (in the year of the observation). 
- **ReturnMay** = the return for the company's stock during May (in the year of the observation). 
- **ReturnJune** = the return for the company's stock during June (in the year of the observation). 
- **ReturnJuly** = the return for the company's stock during July (in the year of the observation). 
- **ReturnAug** = the return for the company's stock during August (in the year of the observation). 
- **ReturnSep** = the return for the company's stock during September (in the year of the observation). 
- **ReturnOct** = the return for the company's stock during October (in the year of the observation). 
- **ReturnNov** = the return for the company's stock during November (in the year of the observation). 
- **PositiveDec** = whether or not the company's stock had a positive return in December (in the year of the observation). This variable takes value 1 if the return was positive, and value 0 if the return was not positive.

For the first 11 variables, the value stored is a proportional change in stock value during that month. For instance, a value of 0.05 means the stock increased in value 5% during the month, while a value of -0.02 means the stock decreased in value 2% during the month.

Let us first obtain the structure f the datase:

```{r}
stocks = read.csv("StocksCluster.csv")
str(stocks)
```

Proportion of observation which is positive can be determined using `table`:

```{r}
table(stocks$PositiveDec)
6324/(6324+5256)
```

The correlation between each component in dataset is:

```{r}
cor(stocks)
```

The maximum correlatin is detected between `ReturnOct` and `ReturnNov`, and is equal to ~ 0.192. Maximum and minumum of returns per month is calculated next:

```{r}
summary(stocks)
```

After short intial exploration of the dataset, we need to devide it into raining and testing set:

```{r}
#install.packages("caTools")
library(caTools)
set.seed(144)
spl = sample.split(stocks$PositiveDec, SplitRatio = 0.7)
stocksTrain = subset(stocks, spl == TRUE)
stocksTest = subset(stocks, spl == FALSE)
```

Next, let us train simple logistic regression model for predictive positive returns at the end of the year:

```{r}
StocksModel = glm(PositiveDec~., family="binomial", data = stocksTrain)
```

The accuracy of our initial model can be obtained from confusion matrix:
```{r}
pred = predict(StocksModel, type="response")
table(stocksTrain$PositiveDec, pred>0.5)
ACC = (990+3640)/nrow(stocksTrain)
ACC
```

The result is not very gooy - accuracy of 0.57 won't cut it. The test predictions of the same model should be even lower:

```{r}
pred = predict(StocksModel, type="response", newdata = stocksTest)
table(stocksTest$PositiveDec, pred>0.5)
ACC = (417+1553)/nrow(stocksTest)
ACC
```

Comparing to baseline model (model that always predicts the most common outcome), this accuracy is really not much of an improvement:

```{r}
table(stocksTest$PositiveDec)
ACC = 1897/(1577+1897)
ACC
```

In order to improve our model, we'll cluster the stocks. The first step in this process is to remove the dependent variable using the following commands:

```{r}
limitedTrain = stocksTrain
limitedTrain$PositiveDec = NULL
limitedTest = stocksTest
limitedTest$PositiveDec = NULL
```

In cases where we have a training and testing set, we'll want to normalize by the mean and standard deviation of the variables in the training set. We can do this by passing just the training set to the preProcess function:

```{r}
#intall.packages("caret")
library(caret)

preproc = preProcess(limitedTrain)
normTrain = predict(preproc, limitedTrain)
normTest = predict(preproc, limitedTest)
```

Now we can run K-Means algorithm:

```{r}
k = 3
set.seed(144)
km = kmeans(normTrain, centers = k)
table(km$cluster)
```

We can now use the `flexclust` package to obtain training set and testing set cluster assignments for our observations

```{r}
# install.packages("flexclust")
library(flexclust)

km.kcca = as.kcca(km, normTrain)
clusterTrain = predict(km.kcca)
clusterTest = predict(km.kcca, newdata=normTest)
```

Similarly to previous studies, we'll devide the dataset according to obtained clusters:

```{r}
stockTrain1 = subset(stocksTrain, clusterTrain == 1)
stockTrain2 = subset(stocksTrain, clusterTrain == 2)
stockTrain3 = subset(stocksTrain, clusterTrain == 3)

stocksTest1 = subset(stocksTest, clusterTest == 1)
stocksTest2 = subset(stocksTest, clusterTest == 2)
stocksTest3 = subset(stocksTest, clusterTest == 3)
```

From `mean(stocksTrain1$PositiveDec)`, `mean(stocksTrain2$PositiveDec)`, and `mean(stocksTrain3$PositiveDec)`, we see that stocksTrain1 has the observations with the highest average value of the dependent variable. Let us build logistic models for which predict PositiveDec using all the other variables as independent variables that use developed subset of the training set.

```{r}
StocksModel1 = glm(PositiveDec~., family="binomial", data = stockTrain1)
StocksModel2 = glm(PositiveDec~., family="binomial", data = stockTrain2)
StocksModel3 = glm(PositiveDec~., family="binomial", data = stockTrain3)

summary(StocksModel1)
summary(StocksModel2)
summary(StocksModel3)
```

We can now build test prediction on these models:

```{r}
pred1 = predict(StocksModel1, type="response", newdata = stocksTest1)
table(stocksTest1$PositiveDec, pred1>0.5)
ACC1 = (30+774)/nrow(stocksTest1)
ACC1
#############
pred2 = predict(StocksModel2, type="response", newdata = stocksTest2)
table(stocksTest2$PositiveDec, pred2>0.5)
ACC2 = (388+757)/nrow(stocksTest2)
ACC2
#############
pred3 = predict(StocksModel3, type="response", newdata = stocksTest3)
table(stocksTest3$PositiveDec, pred3>0.5)
ACC3 = (49+13)/nrow(stocksTest3)
ACC3
```

Finally, to compute the overall test-set accuracy of the cluster-then-predict approach, we can combine all the test-set predictions into a single vector and all the true outcomes into a single vector:

```{r}
AllPredictions = c(pred1, pred2, pred3)
AllOutcomes = c(stocksTest1$PositiveDec, stocksTest2$PositiveDec, stocksTest3$PositiveDec)
table(AllOutcomes, AllPredictions>0.5)
ACC_Total = (467+1544)/length(AllOutcomes)
ACC_Total
```

We see a modest improvement over the original logistic regression model. Since predicting stock returns is a notoriously hard problem, this is a good increase in accuracy. By investing in stocks for which we are more confident that they will have positive returns (by selecting the ones with higher predicted probabilities), this cluster-then-predict model can give us an edge over the original logistic regression model.